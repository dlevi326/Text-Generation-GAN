{"organizations": [], "uuid": "9d7dcfd6081215a9c7853f3c88df4ef6b9074b3f", "thread": {"social": {"gplus": {"shares": 0}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 0, "shares": 0, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "phys.org", "main_image": "http://cdn.phys.org/newman/gfx/news/2015/1-aisystemsolv.jpg", "site_section": "http://phys.org/rss-feed/technology-news/computer-sciences/", "section_title": "Computer Science News | Technology News | Computer Science Technology | Computer Sciences", "url": "http://phys.org/news/2015-09-ai-sat-geometry-average-human.html", "country": "US", "title": "AI system solves SAT geometry questions as well as average human test taker", "performance_score": 0, "site": "phys.org", "participants_count": 0, "title_full": "AI system solves SAT geometry questions as well as average human test taker", "spam_score": 0.0, "site_type": "news", "published": "2015-09-21T17:04:00.000+03:00", "replies_count": 0, "uuid": "9d7dcfd6081215a9c7853f3c88df4ef6b9074b3f"}, "author": "", "url": "http://phys.org/news/2015-09-ai-sat-geometry-average-human.html", "ord_in_thread": 0, "title": "AI system solves SAT geometry questions as well as average human test taker", "locations": [], "entities": {"persons": [], "locations": [], "organizations": []}, "highlightText": "", "language": "english", "persons": ["Aaron Escobar", "Jennifer Langston Credit"], "text": "AI system solves SAT geometry questions as well as average human test taker September 21, 2015 by Jennifer Langston Credit: Aaron Escobar, flickr \nThe Allen Institute for Artificial Intelligence (AI2) announced today it has created an artificial intelligence (AI) system that can solve SAT geometry questions as well as the average American 11th-grade student, a breakthrough in AI research. \nThis system, called GeoS, uses a combination of computer vision to interpret diagrams, natural language processing to read and understand text, and a geometric solver to achieve 49 percent accuracy on geometry questions from the official SAT tests. If these results were extrapolated to the entire Math SAT test, the computer roughly achieved an SAT score of 500 (out of 800), the average test score for 2015. \nA paper outlining the research, entitled \" Solving Geometry Problems: Combining Text and Diagram Interpretation ,\" was a joint effort between University of Washington Computer Science & Engineering department and AI2. \nThese results, presented at the Conference on Empirical Methods in Natural Language Processing (EMNLP) in Lisbon, Portugal, were achieved by GeoS solving unaltered SAT questions that it had never seen before and that required an understanding of: Implicit relationships The relationships between diagrams and Natural-Language text \nA demonstration of the system's problem solving is available here: geometry.allenai.org \n\"Unlike the Turing Test, standardized tests such as the SAT provide us today with a way to measure a machine's ability to reason and to compare its abilities with that of a human,\" said Oren Etzioni, CEO of AI2. \"Much of what we understand from text and graphics is not explicitly stated, and requires far more knowledge than we appreciate. Creating a system to be able to successfully take these tests is challenging, and we are proud to achieve these unprecedented results.\" \nSaid Ali Farhadi, senior research manager for Vision at AI2 and assistant professor of computer science and engineering at UW, \"We are excited about GeoS's performance on real-world tasks. Our biggest challenge was converting the question to a computer-understandable language. One needs to go beyond standard pattern-matching approaches for problems like solving geometry questions that require in-depth understanding of text, diagram, and reasoning.\" An new AI system called GeoS was able to solve SAT questions such as these as well as the average American 11th grader. Credit: AI2/University of Washington \nHow GeoS Works \nGeoS is the first end-to-end system that solves SAT plane geometry problems. It does this by first interpreting a geometry question by using the diagram and text in concert to generate the best possible logical expressions of the problem, which it sends to a geometric solver to solve. Then it compares that answer to the multiple choice answers for that question. \nThis process is complicated by the fact that SAT questions contain many unstated assumptions. \nFor example, for this sample SAT question: \nIn the diagram below, circle O has a radius of 5, and CE = 2. Diameter AC is perpendicular to chord BD at E. What is the length of BD? \nThere are several unstated assumptions, such as the fact that lines BD and AC intersect at E, that \"circle O has a radius of 5\" is the same as \"circle O radius equals 5\" and that the drawing may or may not be to scale. \nGeoS' accuracy was much higher on questions it was confident enough to answer, which is an important dimension of learning. Today, GeoS can solve plane geometry questions; AI2 is moving to solve the full set of Math SAT questions in the next three years. \nAs part of AI2's commitment to sharing its research for the common good, all data sets and software are available for other researchers to use. See http://www.allenai.org/data.html . \nAI2 is also building systems that can tackle science tests, which require a knowledge base that includes elements of the unstated, common sense knowledge that humans generate over their lives. This Aristo project is described here: http://allenai.org/aristo.html .", "external_links": [], "published": "2015-09-21T17:04:00.000+03:00", "crawled": "2015-09-21T21:35:46.535+03:00", "highlightTitle": ""}