{"organizations": [], "uuid": "f6dd6378d684123d9afd2874275b66584fde453a", "thread": {"social": {"gplus": {"shares": 0}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 0, "shares": 0, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "theconversation.com", "main_image": "https://62e528761d0685343e1c-f3d1b99a743ffa4142d9d7f1978d9686.ssl.cf2.rackcdn.com/files/93607/width1356x668/image-20150902-13425-19reur.jpg", "site_section": "http://theconversation.com/au/technology", "section_title": "Science + Technology – Views &amp; Research – The Conversation", "url": "http://theconversation.com/why-we-need-a-legal-definition-of-artificial-intelligence-46796", "country": "US", "title": "Why we need a legal definition of artificial intelligence", "performance_score": 0, "site": "theconversation.com", "participants_count": 6, "title_full": "Why we need a legal definition of artificial intelligence", "spam_score": 0.0, "site_type": "news", "published": "2015-09-03T04:03:00.000+03:00", "replies_count": 6, "uuid": "13f75812b9c3907781c8fb2a03b6554f1a13ae91"}, "author": "theconversation.com", "url": "http://theconversation.com/why-we-need-a-legal-definition-of-artificial-intelligence-46796#post-0", "ord_in_thread": 0, "title": "Why we need a legal definition of artificial intelligence", "locations": [], "entities": {"persons": [], "locations": [], "organizations": []}, "highlightText": "", "language": "english", "persons": [], "text": "When we talk about artificial intelligence (AI) – which we have done lot recently, including my outline on The Conversation of liability and regulation issues – what do we actually mean?\nAI experts and philosophers are beavering away on the issue. But having a usable definition of AI – and soon – is vital for regulation and governance because laws and policies simply will not operate without one.\nThis definition problem crops up in all regulatory contexts, from ensuring truthful use of the term “AI” in product advertising right through to establishing how next-generation automated weapons systems (AWSs) are treated under the laws of war.\nTrue, we may eventually need more than one definition (just as “goodwill” means different things in different contexts). But we have to start somewhere so, in the absence of a regulatory definition at the moment, let’s get the ball rolling.\nDefining the terms: artificial and intelligence For regulatory purposes, “artificial” is, hopefully, the easy bit. It can simply mean “not occurring in nature or not occurring in the same form in nature”. Here, the alternative given after the “or” allows for the possible future use of modified biological materials.\nThis, then, leaves the knottier problem of “intelligence”.\nFrom a philosophical perspective, “intelligence” is a vast minefield, especially if treated as including one or more of “consciousness”, “thought”, “free will” and “mind”. Although traceable back to at least Aristotle’s time, profound arguments on these Big Four concepts still swirl around us.\nIn 2014, seeking to move matters forward, Dmitry Volkov, a Russian technology billionaire, convened a summit on board a yacht of leading philosophers, including Daniel Dennett, Paul Churchland and David Chalmers.\nPerhaps unsurprisingly, no consensus was reached, and Chalmers suggested that it was unlikely to emerge within the next century.\nFortunately for would-be regulators, though, the philosophical arguments might be sidestepped, at least for a while. Let’s take a step back and ask what a regulator’s immediate interest is here?\nI would say that it is the work products of AI scientists and engineers, and any public welfare or safety risks that might arise from those products.\nLogically, then, it is the way that the majority of AI scientists and engineers treat “intelligence” that is of most immediate concern.\nIntelligence and the AI community Until the mid 2000s, there was a tendency in the AI community to contrast artificial intelligence with human intelligence, an action that merely passed the buck to psychologists.\nIn November 2007, John McCarthy , an AI pioneer at Stanford University, addressed this issue :\nQ: Isn’t there a solid definition of intelligence that doesn’t depend on relating it to human intelligence?\nA: Not yet. The problem is that we cannot yet characterize in general what kinds of computational procedures we want to call intelligent. We understand some of the mechanisms of intelligence and not others.\nIt was partly because of this difficulty that much research effort had been redirected from artificial general intelligence (AGI) to artificial narrow intelligence (ANI).\nBut just after McCarthy wrote this, a general “human-independent” definition of “intelligence” emerged. Alongside the formalisation of a universal algorithmic entity called “ AIXI ”, Marcus Hutter (now at ANU) and Shane Legg (now at Google DeepMind) proposed the following informal definition to supersede those that they had previously catalogued :\nIntelligence measures an agent’s ability to achieve goals in a wide range of environments.\nThis informal definition signposts things that a regulator could manage, establishing and applying objective measures of ability (as defined) of an entity in one or more environments (as defined). The core focus on achievement of goals also elegantly covers other intelligence-related concepts such as learning, planning and problem solving.\nBut many hurdles remain.\nFirst, the informal definition may not be directly usable for regulatory purposes because of AIXI’s own underlying constraints. One constraint, often emphasised by Hutter, is that AIXI can only be “approximated” in a computer because of time and space limitations.\nAnother constraint is that AIXI lacks a “self-model” (but a recently proposed variant called “ reflective AIXI ” may change that).\nSecond, for testing and certification purposes, regulators have to be able to treat intelligence as something divisible into many sub-abilities (such as movement, communication, etc.). But this may cut across any definition based on general intelligence.\nFrom a consumer perspective, this is ultimately all a question of drawing the line between a system defined as displaying actual AI, as opposed to being just another programmable box.\nIf we can jump all the hurdles, there will be no time for quiet satisfaction. Even without the Big Four, increasingly capable and ubiquitous AI systems will have a huge effect on society over the coming decades, not least for the future of employment .\nBut if the Big Four do ever (seem to) show up in AI systems, we can safely say that we’ll need not just a yacht of philosophers, but an entire regatta.\nArtificial intelligence Regulation Robots You might also like “Looks like there’s an unexpected item in the bagging area, puny human.” bagogames Mini-megalomaniac AI is already all around us, but it won’t get further without our help", "external_links": ["https://62e528761d0685343e1c-f3d1b99a743ffa4142d9d7f1978d9686.ssl.cf2.rackcdn.com/files/83578/width1012x668/image-20150601-6955-15xyw5y.jpg", "http://www-cs.stanford.edu/content/professor-john-mccarthy", "http://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf", "https://intelligence.org/files/ReflectiveSolomonoffAIXI.pdf", "http://arxiv.org/abs/0712.3329", "http://www-formal.stanford.edu/jmc/whatisai/node1.html", "http://www.theguardian.com/science/2015/jan/21/-sp-why-cant-worlds-greatest-minds-solve-mystery-consciousness", "http://arxiv.org/pdf/0706.3639v1.pdf", "http://profiles.arts.monash.edu.au/rob-sparrow/download/20-seconds-to-comply.pdf"], "published": "2015-09-03T04:03:00.000+03:00", "crawled": "2015-09-03T06:39:25.442+03:00", "highlightTitle": ""}