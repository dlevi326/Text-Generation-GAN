{"organizations": [], "uuid": "f78a65407c7ba74361eea5b46427cc6361781453", "thread": {"social": {"gplus": {"shares": 0}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 0, "shares": 0, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "radar.oreilly.com", "main_image": "http://cdn.oreillystatic.com/radar/images/people/photo_gbrail_m.jpg", "site_section": "http://strata.oreilly.com/feed", "section_title": "O'Reilly Radar - Insight, analysis, and research about emerging technologies", "url": "http://radar.oreilly.com/2015/09/building-apis-with-swagger.html", "country": "US", "title": "Building APIs with Swagger", "performance_score": 0, "site": "oreilly.com", "participants_count": 1, "title_full": "Building APIs with Swagger", "spam_score": 0.0, "site_type": "news", "published": "2015-09-30T18:48:00.000+03:00", "replies_count": 0, "uuid": "f78a65407c7ba74361eea5b46427cc6361781453"}, "author": "Greg Brail", "url": "http://radar.oreilly.com/2015/09/building-apis-with-swagger.html", "ord_in_thread": 0, "title": "Building APIs with Swagger", "locations": [], "entities": {"persons": [], "locations": [], "organizations": []}, "highlightText": "", "language": "english", "persons": [], "text": "@ ApiParam ( value = \"UUID of the element\" , required = true ) @ PathParam ( \"uuid\" ) Sting uuid ) \nOnce the code is written, a separate tool introspects the source code to generate documentation, client­-side code, and other artifacts. (Sometimes this tool happens at runtime, as with the original Swagger, and sometimes it happens at deployment time or compile time, but the effect is the same.) \nThe advantage of this approach is that the code and documentation are never out of sync, as long as the documentation is always re­-generated (and updated on the web site or wherever) whenever the code changes. \nHowever, with this approach, there is no formal mechanism for the developer of the code to have a “conversation” with other parties regarding the API, other than via the source code itself. \nThere’s also not a great mechanism for the technical writer to tie quality documentation to the structure of the API. If the writing team wants to use the generated documentation as the basis of the “real” documentation, then that means that the tech writers need access to the code base in order to update the bits of documentation that are kept in the code. \nWith this kind of tool, the only real option is to have all parties collaborate on the code base itself, and to keep abreast of any changes (and if necessary, to stop them before going live). This works particularly poorly in closed source situations, or even in open source situations when the code that runs the API is sufficiently large or complex that the average “user” cannot be expected to keep track of it. \nIt is similarly cumbersome if the technical writers don’t want to have to learn how to build and test the code in order to check in documentation changes. \nAlso, what if you don’t want the documentation to match the code? Perhaps there are parts of the API that you don’t want to document, at least not right away. Or perhaps there is a need to change the docs without doing a code release. All of these things end up leading to the conclusion that, for “real products” at least, the API docs can’t simply be generated from the code and then put up on the website for everyone to see. Generated source code: The API generates the code \nThe second category is represented by “IDL” (Interface Definition Language) systems such as SOAP, CORBA and the many RPC systems that have been developed over the years. \nIn these types of systems, the interface is formally defined in a separate file, and then used to generate client­ and server-­side “stubs” that connect the bits sent over the network to actual code written by a developer. Developers on both sides then incorporate their stubs into the source code that they build and ship. \nThe advantages of this approach are performance and simplicity for the developer. Since the stub code is generated at compile time, the generator can take care to make it efficient and pre­-compile it, and starting it up at runtime is fast since there is no need to parse the IDL. Furthermore, once the developer specifies the IDL, the interface that she needs to code to in order to make the stubs invoke her code is typically simple. \nHowever, with generated stub code, the possibility of the code falling out of sync with the specification is still present, since the synchronization only happens when the code is re-generated. Good RPC systems make this simple, whereas more primitive systems that generate code and expect the user to modify it are much harder to maintain. \nIn addition, the same documentation problems as before still exist. If the IDL is annotated to contain the docs, and then generate the “real” docs from the IDL, then how do is everything kept in sync? By re­-generating the stubs and re-­building the product just because the docs were changed? If not, is there a risk that the re­-generation is missed the next time a “real” change is made, resulting in clients and servers that don’t interoperate? \nPlus, dealing with generated code in a modern software development process is painful. Do you manually run the stub generator and check in the results? If so, you had better not forget to run it, and remember never to modify the generated code manually. Or do you make the build system run it on every build? That may mean creating and testing custom build steps for whatever build system you use. \nOne advantage of this mechanism is the performance gain received by building the stubs at build time, rather than when the system starts up. That made a lot of sense in the 1990s. But with today’s immensely faster CPUs, the time needed to parse and validate an IDL file and generate a run­time representation is just not significant, and languages like JavaScript (and even Java) are dynamic enough that they can work without loads of generated code. \nNode.js itself is a great example; even a simple Node.js­based server loads and compiles tens of thousands of lines of JavaScript code when it is first started, and yet it is rare for the startup of a Node.js­based application to take more than one or two seconds. (And yet, even a simple Java app, fully compiled, takes seconds if not more to start ­­ but I digress.) No connection at all: The code is the API \nMany other server­side frameworks, especially popular Node.js­based frameworks like Express, do not have the concept of an API description at all. For instance, in the Express framework, the developer simply wires JavaScript functions to URI paths and verbs using a set of function calls, and then Express handles the rest. \nThis is a simple example of code written in the Express framework: // GET method route api.get('/', function (req, res) { res.send('GET request to the homepage') }) // POST method route api.post('/', function (req, res) { res.send('POST request to the homepage') }) 1", "external_links": [], "published": "2015-09-30T18:48:00.000+03:00", "crawled": "2015-09-30T20:27:55.233+03:00", "highlightTitle": ""}