{"organizations": [], "uuid": "8017126e95be71ff48f7eb458428aea7e8210033", "thread": {"social": {"gplus": {"shares": 0}, "pinterest": {"shares": 0}, "vk": {"shares": 0}, "linkedin": {"shares": 0}, "facebook": {"likes": 0, "shares": 0, "comments": 0}, "stumbledupon": {"shares": 0}}, "site_full": "www.csmonitor.com", "main_image": "http://images.csmonitor.com/csm/2015/09/936473_1_0922-Math-students_standard.jpg?alias=standard_900x600", "site_section": "http://www.csmonitor.com/Technology", "section_title": "Technology - The Christian Science Monitor - CSMonitor.com", "url": "http://www.csmonitor.com/Technology/2015/0922/This-AI-computer-can-beat-students-at-SAT-geometry-questions", "country": "US", "title": "This AI computer can beat students at SAT geometry questions", "performance_score": 0, "site": "csmonitor.com", "participants_count": 1, "title_full": "This AI computer can beat students at SAT geometry questions - CSMonitor.com", "spam_score": 0.0, "site_type": "news", "published": "2015-09-22T03:00:00.000+03:00", "replies_count": 0, "uuid": "8017126e95be71ff48f7eb458428aea7e8210033"}, "author": "Annika Fredrikson", "url": "http://www.csmonitor.com/Technology/2015/0922/This-AI-computer-can-beat-students-at-SAT-geometry-questions", "ord_in_thread": 0, "title": "This AI computer can beat students at SAT geometry questions", "locations": [], "entities": {"persons": [], "locations": [], "organizations": []}, "highlightText": "", "language": "english", "persons": [], "text": "In 2014, the average SAT test taker correctly answered answered 49 percent of the test's math questions. Today, a new software program is now close to doing the same.\nIn a pape r published Monday , researchers at the Allen Institute for Artificial Intelligence (AI2) and the University of Washington revealed that their artificial intelligence (AI) system, known as GeoSolver, or GeoS for short, is able to answer “unseen and unaltered” geometry problems on par with humans.\nAccording to a report released by College Board, the average SAT math score in 2014 was 513. Though GeoS has only been tested on geometry questions, if the system’s accuracy was extrapolated, GeoS would have scored a 500.\nRecommended: 1912 eighth grade exam: Could you make it to high school in 1912? Using a combination of computer vision and natural language processing, GeoS can interpret diagrams and process text that it then feeds into a geometric solver that analyzes the input and selects the best multiple choice answer.\nTest your knowledge 1912 eighth grade exam: Could you make it to high school in 1912? In Pictures Where the high-tech jobs are Photos day 09/23 “Our method consists of two steps : interpreting a geometry question by deriving a logical expression that represents the meaning of the text and the diagram, and solving the geometry question by checking the satisfiablity of the derived logical expression,” write the researchers in their most recent paper.\nThe difficulty lies in analyzing the diagrams, which in the context of geometry problems, usually provide information missing from the textual descriptions.\nIn alignment with AI2’s mission of AI research for the common good, all of GeoS source code and test data is available online. You can also watch a demo of how GeoS works.\nWhile the test results may not seem that impressive on their own, it’s important to consider how much work it takes to achieve even these “average” results.\nRecommended: On Pinterest? Follow the Monitor now! “Much of what we understand from text and graphics is not explicitly stated, and requires far more knowledge than we appreciate ,\" AI2 CEO Oren Etzioni said in a press release. \"Creating a system to be able to successfully take these tests is challenging, and we are proud to achieve these unprecedented results.\"\nGeoS is in fact, the first end-to-end system that can solve these types of plane geometry problems. But beyond that, GeoS signifies development in a machine's ability to reason as opposed to merely perceiving input and recognizing patterns, which AI systems such as Apple's Siri have advanced in recent years.\nMany experts say we’re still years away from the biggest AI advances. “Although they can do some individual tasks as well as or even better than humans, technology still cannot approach the complex thinking that humans have,” says Microsoft Allison Linn.\nBut GeoS does represent significant progress, even if minimal in the grand scheme of AI.\nThere’s no doubt that replicating human intelligence is complicated, but “ we’re taking baby steps in that direction,” wrote Fortune's Derrick Harris.", "external_links": ["http://geometry.allenai.org/demo/", "http://geometry.allenai.org/", "http://fortune.com/2015/09/21/computer-artificial-intelligence-math/", "http://allenai.org/index.html", "https://www.pinterest.com/csmonitor/", "http://www.washington.edu/news/2015/09/21/ai-system-solves-sat-geometry-questions-as-well-as-average-human-test-taker/", "https://secure-media.collegeboard.org/digitalServices/pdf/sat/sat-percentile-ranks-crit-reading-math-writing-2014.pdf", "http://geometry.allenai.org/assets/emnlp2015.pdf", "http://blogs.microsoft.com/next/2015/07/06/the-future-of-artificial-intelligence-myths-realities-and-aspirations/", "https://secure-media.collegeboard.org/digitalServices/pdf/sat/TotalGroup-2014.pdf"], "published": "2015-09-22T03:00:00.000+03:00", "crawled": "2015-09-26T23:42:29.682+03:00", "highlightTitle": ""}